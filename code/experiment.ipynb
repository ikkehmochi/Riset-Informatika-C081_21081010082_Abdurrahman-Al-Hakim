{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 16\n",
    "OUTPUT_SHAPE=5\n",
    "MODEL=\"efficientnet_v2_L\"\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "dataset_path=os.path.join(os.getcwd(), \"Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()])\n",
    "dataset=datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))  \n",
    "val_size = int(0.1 * len(dataset)) \n",
    "test_size = len(dataset) - train_size - val_size \n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n",
      "Rectification enabled in AdaBelief\n",
      "[INFO] Created SummaryWriter, saving to: runs\\2024-12-17\\.fficientnet_v2_L\\x...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c298de1f1f834b3b93ca0674fe000047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.6346 | train_acc: 0.2514 | val_loss: 1.5918 | val_acc: 0.3092 | test_loss: 1.5891 | test_acc: 0.3203\n",
      "Epoch: 2 | train_loss: 1.4803 | train_acc: 0.3771 | val_loss: 4.4904 | val_acc: 0.4699 | test_loss: 6.7856 | test_acc: 0.3828\n",
      "Epoch: 3 | train_loss: 1.2605 | train_acc: 0.4625 | val_loss: 13.9433 | val_acc: 0.4810 | test_loss: 22.3837 | test_acc: 0.4688\n",
      "Epoch: 4 | train_loss: 1.1919 | train_acc: 0.4969 | val_loss: 1.5492 | val_acc: 0.5480 | test_loss: 1.4369 | test_acc: 0.4453\n",
      "Epoch: 5 | train_loss: 1.0280 | train_acc: 0.5410 | val_loss: 6.5305 | val_acc: 0.5513 | test_loss: 2.0853 | test_acc: 0.4844\n",
      "Epoch: 6 | train_loss: 1.0149 | train_acc: 0.5642 | val_loss: 2.1367 | val_acc: 0.4475 | test_loss: 2.3913 | test_acc: 0.4922\n",
      "Epoch: 7 | train_loss: 0.9522 | train_acc: 0.5913 | val_loss: 3.1634 | val_acc: 0.4420 | test_loss: 2.5016 | test_acc: 0.5156\n",
      "Epoch: 8 | train_loss: 0.8260 | train_acc: 0.5990 | val_loss: 1.2523 | val_acc: 0.4732 | test_loss: 1.0565 | test_acc: 0.4922\n",
      "Epoch: 9 | train_loss: 0.7831 | train_acc: 0.6549 | val_loss: 0.6184 | val_acc: 0.7042 | test_loss: 0.6593 | test_acc: 0.6875\n",
      "Epoch: 10 | train_loss: 0.6686 | train_acc: 0.7153 | val_loss: 0.5844 | val_acc: 0.7042 | test_loss: 0.7181 | test_acc: 0.6953\n",
      "Epoch: 11 | train_loss: 0.5915 | train_acc: 0.7361 | val_loss: 0.6096 | val_acc: 0.7176 | test_loss: 0.7320 | test_acc: 0.7188\n",
      "Epoch: 12 | train_loss: 0.5556 | train_acc: 0.7521 | val_loss: 0.7173 | val_acc: 0.7143 | test_loss: 0.8260 | test_acc: 0.6641\n",
      "Epoch: 13 | train_loss: 0.6337 | train_acc: 0.7333 | val_loss: 0.4672 | val_acc: 0.7455 | test_loss: 0.5520 | test_acc: 0.7812\n",
      "Epoch: 14 | train_loss: 0.5223 | train_acc: 0.7892 | val_loss: 0.4609 | val_acc: 0.7768 | test_loss: 0.6754 | test_acc: 0.7422\n",
      "Epoch: 15 | train_loss: 0.5301 | train_acc: 0.7837 | val_loss: 0.5358 | val_acc: 0.7980 | test_loss: 0.6443 | test_acc: 0.7109\n",
      "Epoch: 16 | train_loss: 0.4664 | train_acc: 0.8066 | val_loss: 0.4657 | val_acc: 0.7589 | test_loss: 0.5932 | test_acc: 0.7656\n",
      "Epoch: 17 | train_loss: 0.5018 | train_acc: 0.8160 | val_loss: 0.6530 | val_acc: 0.7612 | test_loss: 0.8274 | test_acc: 0.6719\n",
      "Epoch: 18 | train_loss: 0.4679 | train_acc: 0.8069 | val_loss: 0.5896 | val_acc: 0.7199 | test_loss: 0.7555 | test_acc: 0.7422\n",
      "Epoch: 19 | train_loss: 0.4493 | train_acc: 0.8264 | val_loss: 1.8613 | val_acc: 0.5089 | test_loss: 1.7046 | test_acc: 0.5469\n",
      "Epoch: 20 | train_loss: 0.4309 | train_acc: 0.8208 | val_loss: 0.4215 | val_acc: 0.8080 | test_loss: 0.4750 | test_acc: 0.7734\n",
      "[INFO] Saving model to: Models\\.fficientnet_v2_L_0.pt\n"
     ]
    }
   ],
   "source": [
    "import engine\n",
    "import utils\n",
    "from adabelief_pytorch import AdaBelief\n",
    "import ensembleModel\n",
    "for x in range(0, 1):\n",
    "\n",
    "    model1=torchvision.models.efficientnet_v2_s().to(DEVICE)\n",
    "    model1.classifier=torch.nn.Sequential(torch.nn.Dropout(p=0.2, inplace=True), torch.nn.Linear(in_features=1280, out_features=OUTPUT_SHAPE, bias=True).to(DEVICE))\n",
    "    model1.load_state_dict(torch.load(r\"Models\\.fficientnet_v2_s_0.pt\", weights_only=True))\n",
    "\n",
    "    model2=torchvision.models.efficientnet_v2_s().to(DEVICE)\n",
    "    model2.classifier=torch.nn.Sequential(torch.nn.Dropout(p=0.2, inplace=True), torch.nn.Linear(in_features=1280, out_features=OUTPUT_SHAPE, bias=True).to(DEVICE))\n",
    "    model2.load_state_dict(torch.load(r\"Models\\.fficientnet_v2_s_1.pt\", weights_only=True))\n",
    "    model=ensembleModel.AdaptiveEnsembleModel(model1=model1, model2=model2, num_classes=OUTPUT_SHAPE).to(DEVICE)\n",
    "    for param in model.model1.features.parameters():\n",
    "        param.requires_grad=False\n",
    "    for param in model.model2.features.parameters():\n",
    "        param.requires_grad=False\n",
    "    model.adaptive_layer.requires_grad=True\n",
    "    loss_fn=torch.nn.CrossEntropyLoss()\n",
    "    optimizer=AdaBelief(params=model.parameters())\n",
    "    engine.train(model=model,\n",
    "                            train_dataloader=train_loader,\n",
    "                            val_dataloader=val_loader,\n",
    "                            test_dataloader=test_loader,\n",
    "                            loss_fn=loss_fn,\n",
    "                            optimizer=optimizer,\n",
    "                            epochs=NUM_EPOCHS,\n",
    "                            writer=engine.create_writer(experiment_name=MODEL,\n",
    "                                                        model_name=\"x\",\n",
    "                                                        extra=f\"\"),\n",
    "                            device=DEVICE)\n",
    "    utils.save_model(model=model,\n",
    "                                target_dir=f\"Models/\",\n",
    "                                model_name=f\"{MODEL}_{x}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
